# ECF1 â€“ Plateforme Data Engineering & Analytics

## ğŸ“Œ PrÃ©sentation du projet

Ce projet consiste Ã  concevoir et implÃ©menter une **plateforme de traitement de donnÃ©es complÃ¨te**, depuis la collecte de donnÃ©es hÃ©tÃ©rogÃ¨nes jusquâ€™Ã  leur exploitation analytique, en respectant les bonnes pratiques de **Data Engineering**, de **traÃ§abilitÃ©**, et de **conformitÃ© RGPD**.

La plateforme intÃ¨gre :
- des donnÃ©es issues de **scraping web**
- des donnÃ©es **Excel partenaires**
- une **API externe** de gÃ©ocodage
- un stockage relationnel analytique
- un stockage objet pour les fichiers volumineux (images)

---

## ğŸ—ï¸ Architecture globale

Lâ€™architecture repose sur un modÃ¨le **Lakehouse** structurÃ© en couches :

- **Bronze** : donnÃ©es brutes, historisÃ©es, immuables
- **Silver** : donnÃ©es nettoyÃ©es, normalisÃ©es, conformes RGPD
- **Gold** : donnÃ©es prÃªtes Ã  lâ€™analyse dans PostgreSQL
- **Object Storage** : images stockÃ©es dans MinIO (S3 compatible)

Les traitements sont orchestrÃ©s par un **pipeline Python conteneurisÃ© avec Docker**.

---

## ğŸ§± Technologies utilisÃ©es

| Usage | Technologie | Justification |
|-----|-----------|--------------|
| Orchestration | Python | LisibilitÃ©, Ã©cosystÃ¨me data mature |
| Scraping | Requests, BeautifulSoup | LÃ©ger et contrÃ´lÃ© |
| Stockage brut | Fichiers (JSON, XLSX) | TraÃ§abilitÃ© et audit |
| Transformation | Pandas | Validation et nettoyage |
| Base analytique | PostgreSQL 16 | SQL standard, contraintes fortes |
| Object storage | MinIO | Stockage scalable des images |
| Conteneurisation | Docker / Docker Compose | ReproductibilitÃ© |
| Logging | logging (Python) | ObservabilitÃ© du pipeline |

---

## ğŸ“‚ Organisation du projet

```text
ECF1/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ config.yaml
|
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ run_id=YYYY-MM-DDTHH-MM-SS/
â”‚   |â”€â”€ silver/
â”‚   |   â””â”€â”€ run_id=YYYY-MM-DDTHH-MM-SS/
â”‚   â””â”€â”€ partenaire_librairies.xlsx
|
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ DAT.md                    # Dossier Architecture Technique
â”‚   â””â”€â”€ RGPD_CONFORMITE.md        # ConformitÃ© RGPD
|
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/
â”‚   |   â”œâ”€â”€ adresse_api.py                    
|   |   â”œâ”€â”€ book_scraper.py
|   |   â”œâ”€â”€ partners_excel.py
â”‚   |   â””â”€â”€ quotes_scraper.py        
â”‚   â”œâ”€â”€ loaders/
â”‚   |   â”œâ”€â”€ bronze_files.py                    
|   |   â”œâ”€â”€ bronze_writer.py
|   |   â”œâ”€â”€ postgres_loader.py
â”‚   |   â””â”€â”€ silver_writer.py
â”‚   â”œâ”€â”€ transformers/
â”‚   |   â”œâ”€â”€ books_transformer.py                    
|   |   â”œâ”€â”€ partners_cleaner.py
â”‚   |   â””â”€â”€ quotes_transformer.py
â”‚   â”œâ”€â”€ storage/                    
â”‚   |   â””â”€â”€ minio_client.py 
â”‚   â”œâ”€â”€ utils/                    
â”‚   |   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql
â”‚   â””â”€â”€ analyses.sql
â”‚
â”œâ”€â”€ logs/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ ECF-DataPulse-MultiSources.md
â””â”€â”€ requirements.txt

```

âš ï¸ Le dossier data/gold nâ€™existe pas :
la couche Gold correspond Ã  la base PostgreSQL, pas Ã  des fichiers.

## ğŸ”„ Pipeline de traitement

Chaque exÃ©cution du pipeline suit les Ã©tapes suivantes :

### 1ï¸âƒ£ Snapshot Bronze
- Copie horodatÃ©e des sources (Excel partenaires, scraping web)
- Conservation des donnÃ©es brutes Ã  des fins de traÃ§abilitÃ© et dâ€™audit

### 2ï¸âƒ£ Extraction
- Lecture des donnÃ©es depuis la couche **Bronze**
- Aucune modification des donnÃ©es sources

### 3ï¸âƒ£ Transformation (Silver)
- Nettoyage des donnÃ©es
- Typage et normalisation
- Application des rÃ¨gles de conformitÃ© **RGPD**

### 4ï¸âƒ£ Chargement (Gold)
- Insertion des donnÃ©es dans **PostgreSQL**
- Application de contraintes dâ€™intÃ©gritÃ© (clÃ©s primaires, clÃ©s Ã©trangÃ¨res, unicitÃ©)

### 5ï¸âƒ£ Enrichissement
- GÃ©ocodage des partenaires via une **API externe**
- Stockage des rÃ©sultats dans des tables dÃ©diÃ©es

### 6ï¸âƒ£ Stockage des images
- Upload des images vers **MinIO** (Object Storage compatible S3)
- Stockage **uniquement de lâ€™URL** de lâ€™image en base de donnÃ©es

Chaque exÃ©cution du pipeline est identifiÃ©e par un **`run_id`**, garantissant une **traÃ§abilitÃ© complÃ¨te** de bout en bout.

---

## ğŸªµ Logging & traÃ§abilitÃ©

Le projet intÃ¨gre un systÃ¨me de **logging structurÃ©** afin dâ€™assurer la traÃ§abilitÃ© complÃ¨te des traitements et de faciliter le diagnostic en cas dâ€™erreur.

### Objectifs du logging

- Suivi des exÃ©cutions du pipeline
- Identification rapide des erreurs ou blocages
- Audit des opÃ©rations rÃ©alisÃ©es (scraping, transformation, chargement)
- Analyse des performances et des volumes traitÃ©s

### Organisation des logs

Les logs sont Ã©crits dans le dossier : /logs

Chaque exÃ©cution du pipeline gÃ©nÃ¨re des messages horodatÃ©s indiquant :

- le `run_id` de lâ€™exÃ©cution
- le dÃ©marrage et la fin de chaque Ã©tape
- les volumes de donnÃ©es traitÃ©s
- les appels aux API externes
- les erreurs ou avertissements Ã©ventuels

### Niveaux de logs utilisÃ©s

- `INFO` : suivi normal du pipeline
- `WARNING` : anomalie non bloquante (ex. adresse non gÃ©ocodable)
- `ERROR` : erreur bloquante entraÃ®nant lâ€™arrÃªt du pipeline

Ce mÃ©canisme garantit une **observabilitÃ© complÃ¨te** du systÃ¨me, indispensable dans un contexte de production ou dâ€™audit.

---

## ğŸ” ConformitÃ© RGPD

Le projet intÃ¨gre la conformitÃ© RGPD dÃ¨s la conception :

- Identification explicite des donnÃ©es personnelles
- Pseudonymisation des donnÃ©es sensibles (hash des contacts)
- SÃ©paration claire entre donnÃ©es sensibles et donnÃ©es analytiques
- PossibilitÃ© de suppression sur demande (droit Ã  lâ€™effacement)
- Absence de donnÃ©es personnelles inutiles dans la couche **Gold**

ğŸ“„ Un document dÃ©diÃ© est fourni :  
**`RGPD_CONFORMITE.md`**

---

## ğŸ“Š Analyses & exploitation

Les donnÃ©es finales sont exploitÃ©es via **PostgreSQL** Ã  lâ€™aide de requÃªtes analytiques dÃ©montrant :

- Des requÃªtes dâ€™agrÃ©gation
- Des jointures multi-sources
- Des fonctions de fenÃªtrage (window functions)
- Des classements de type **Top-N**

ğŸ“„ Les requÃªtes sont disponibles dans :  
**`sql/analyses.sql`**

# â–¶ï¸ Lancer le projet

## PrÃ©requis

- Docker  
- Docker Compose  

## Lancement

```bash
docker compose up -d
docker compose run --rm pipeline
```

## AccÃ¨s aux services

- **PostgreSQL** : `localhost:5432`
- **pgAdmin** : http://localhost:8080
- **MinIO** : http://localhost:9000 
